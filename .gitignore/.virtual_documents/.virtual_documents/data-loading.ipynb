





!pip install -q rasterio
!pip install -q utm


## Imports

#Standard
import os
import gc
import shutil

#Coordinates
import utm


#Visualizations & Imaging
from PIL import Image
import matplotlib.pyplot as plt


#Data & Math
import numpy as np
import pandas as pd


#Sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

#Pytorch
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models
from torchvision.models import resnet50
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset

#Utilities
import tqdm


# Paths
positive_ruins_coordinates_path = '/kaggle/input/amazon-data/amazon_data.csv'



## Original Data from Kaggle Ruins Locations in the Amazon

## Constructs a csv of Coordinates of ruins and earthworks from datasets found on kaggle


glyph = pd.read_csv('amazon_geoglyphs_sites.csv')
arch_data = pd.read_csv('submit.csv')
casarabe = pd.read_csv('casarabe_sites_utm.csv')
arch_data = arch_data[['x','y']]
arch_data['latitude'] = arch_data['y']
arch_data['longitude'] = arch_data['x']
arch_data = arch_data[['latitude','longitude']]
glyph = glyph[['latitude','longitude']]


#Fixing Warped Entries
glyph.loc[1936,'latitude'],_ = glyph['latitude'][1936].split(' ')
glyph.loc[2063,'latitude'],_ = glyph['latitude'][2063].split(' ')
glyph.loc[2866,'latitude'],_ = glyph['latitude'][2866].split(' ')

#Converting utm to lat/lon
casarabe['easting'] = casarabe['UTM X (Easting)']
casarabe['northing'] = casarabe['UTM Y (Northing)']
casarabe = casarabe[['easting','northing']]
casarabe['latitude'] = casarabe['easting']
casarabe['longitude'] = casarabe['northing']

def convert_to_utm(easting, northing):
  zone_number = 20
  zone_letter = 'S'
  latitude_south, longitude_south = utm.to_latlon(easting, northing, zone_number, northern=False)

  return latitude_south, longitude_south

for i in range(len(casarabe)):
  casarabe.loc[i,'latitude'], casarabe.loc[i,'longitude'] = convert_to_utm(casarabe['easting'][i],casarabe['northing'][i])

casarabe = casarabe[['latitude','longitude']]

#Glyph & arch_data have some crossover:

glyph = glyph.drop_duplicates(subset=['latitude', 'longitude'], keep='first')
arch_data = arch_data.drop_duplicates(subset=['latitude', 'longitude'], keep='first')

glyph['latitude'] = round(glyph['latitude'],4)
glyph['longitude'] = round(glyph['longitude'],3)



for index, row in glyph.iterrows():
  glyph.loc[index,'latitude'] = round(float(glyph['latitude'][index]),4)

holder = pd.concat([glyph,arch_data,casarabe],ignore_index = True)


before = len(holder)
master_table = holder.drop_duplicates(subset=['latitude', 'longitude'], keep='first')
print(len(master_table))
after = len(master_table)

print(f"Duplicated coordinates: {before-after}")
master_table['latitude'] = master_table['latitude'].astype(str)

for index, row in master_table.iterrows():
  # Access the latitude value directly from the 'row' Series
  master_table.loc[index,'latitude'] = float(row['latitude'])

master_table.head()

master_table.to_csv('amazon_data.csv')


##Tile Loading Functions

def generate_ndvi_tile(lat, lon, step = .02):
    """
    Normalized Difference Vegetation Index
    Normalized Difference of Near Infrared (NIR) and Red bands
    NDVI (vegetation index): (NIR - RED)/(NIR + RED)
    lat: latitude, float
    lon: longitude, float
    step: designates tile area, float
 
    """
    ee.Authenticate()
    ee.Initialize()

    #Defining region
    region = ee.Geometry.Rectangle([lon - step, lat - step, lon + step, lat + step])

    #LANDSAT data
    collection = ee.ImageCollection(
        'LANDSAT/LC08/C02/T1_L2') \
        .filterBounds(region) \
        .filterDate('2022-01-01', '2022-12-31') \
        .sort('CLOUD_COVER') \
        .first()

    #Scaling
    def scale_bands(img):
        return img.select(
                    ['SR_B4', 'SR_B5']) \
                  .multiply(0.0000275).add(-0.2) \
                  .rename(['Red', 'NIR'])

    image = scale_bands(collection)

    #calculate NDVI, clipping and producing image
    ndvi = image.normalizedDifference(['NIR', 'Red']).rename('NDVI')
    ndvi = ndvi.clip(region)
    ndvi_array = ndvi.sampleRectangle(region=region, defaultValue=0).get('NDVI').getInfo()
    ndvi_np = np.array(ndvi_array).astype(np.float32).copy()
    ndvi_norm = np.clip((ndvi_np + 1) / 2, 0, 1)
    image = Image.fromarray((ndvi_norm * 255).astype(np.uint8))

    return image

    
def generate_ndbi_tile(lat, lon, step = .02):
    """
    Normalized Difference Built-Up Index
    Normalized Difference of Near Infrared (NIR) and Showt-Wave Infrared (SWIR)
    NDVI (Built-Up Index): (SWIR - NIR) / (SWIR + NIR)
    lat: latitude, float
    lon: longitude, float
    step: designates tile area, float
    """
    ee.Initialize()

    #Defining region
    region = ee.Geometry.Rectangle([lon - step, lat - step, lon + step, lat + step])

    #LANDSAT Data
    landsat = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \
    .filterBounds(region) \
    .filterDate('2020-01-01', '2020-12-31') \
    .filterMetadata('CLOUD_COVER', 'less_than', 50) \
    .sort('CLOUD_COVER') \
    .first()

    #Scaling
    def scale_bands(img):
        return img.select(
                    ['SR_B6', 'SR_B5']) \
                  .multiply(0.0000275).add(-0.2) \
                  .rename(['SWIR', 'NIR'])
    
    
    
    image = scale_bands(landsat)

    #Calculating Index
    ndbi = image.normalizedDifference(['SWIR', 'NIR']).rename('NDBI')

    #Clipping index values to the region
    ndbi = ndbi.clip(region).unmask(0).reproject(crs='EPSG:4326', scale=30)
    
    #To array + resized
    ndbi_array = ndbi.sampleRectangle(region=region, defaultValue=0).get('NDBI').getInfo()
    ndbi_np = np.array(ndbi_array).astype(np.float32).copy()
    ndbi_norm = np.clip((ndbi_np + 1) / 2, 0, 1)

    img = Image.fromarray((ndbi_norm * 255).astype(np.uint8))

    return img



def generate_rgb_tile(lat,lon,step = 0.02, authenticate=False):
    """
    Standard Satellite Imagery 
    lat: latitude, float
    lon: longitude, float
    step: designates tile area, float
    """

    #Optional authentication to generate a session token
    if authenticate:
        ee.Authenticate(auth_mode='notebook', force=True)
    else:
        ee.Authenticate()
    ee.Initialize()

    #Defining region
    region = ee.Geometry.Rectangle([lon-step, lat-step, lon+step, lat+step])

    #Copernicus data
    sentinel = ee.ImageCollection(
        "COPERNICUS/S2_SR_HARMONIZED") \
        .filterBounds(region) \
        .filterDate('2022-01-01', '2022-12-31') \
        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10)) \
        .median() \
        .clip(region)
    
    #URL to get RGB
    url = sentinel.getThumbURL({
        'bands': ['B4', 'B3', 'B2'],  # RGB bands
        'region': region,
        'dimensions': 512,
        'min': 0,
        'max': 3000,
        'format': 'png'
    })

    #Get image, return image and region
    
    response = requests.get(url)
    image_pil = Image.open(io.BytesIO(response.content))
    geo_bounds = {'lat_min': lat-.02, 'lat_max': lat+.02, 'lon_min': lon-.02, 'lon_max':lon+.02}
    return image_pil, geo_bounds
    

def generate_elevation_tile(lat,lon,step = 0.02):
    """
    Elevation in Meters, Topograchic Data from the USGS
    lat: latitude, float
    lon: longitude, float
    step: designates tile area, float
    """
    #Defining region
    aoi = ee.Geometry.BBox(lon-step, lat-step, lon+step, lat+step)
    
    #Specifying dataset
    srtm = ee.Image("USGS/SRTMGL1_003").clip(aoi)
    out_dem = "dem.tif"
    geemap.ee_export_image(
        srtm,
        filename=out_dem,
        scale=30,
        region=aoi,
        file_per_band=False
    )
    
    with rasterio.open(out_dem) as src:
        elevation_array = src.read(1)
        
    #Normalizing
    array_min = elevation_array.min()
    array_ptp = elevation_array.ptp() + 1e-8  # Zeros handling
    normalized = (elevation_array - array_min) / array_ptp

    image = Image.fromarray((normalized * 255).astype(np.uint8))
    
    return image


#Processing Images to Tensors



rgb_preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),  # [3, 224, 224]
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])


#for single bands (NDBI, NDVI, Elevation)
def preprocess_band(pil_img, normalize=True):
    """
    Normalize + resize image bands

    pil_img: PIL Image object
    normalize: bool
    """
    img = pil_img.resize((224, 224))
    arr = np.array(img).astype(np.float32) / 255.0
    arr = arr[np.newaxis, :, :] 

    if normalize:
        mean = arr.mean()
        std = arr.std() if arr.std() > 0 else 1.0
        arr = (arr - mean) / std

    tensor = torch.tensor(arr, dtype=torch.float32)  #[Channels, 224, 224]
    return tensor
    

def tile_to_tensor(tile):
    """
    Produces full tensor ready for feature extraction
    
    """

    
    # RGB first
    rgb_tensor = rgb_preprocess(tile["RGB"])  #[3, 224, 224]
    
    # Scientific bands
    ndvi_tensor = preprocess_band(tile["NDVI"])  #[1, 224, 224]

    ndbi_tensor = preprocess_band(tile["NDBI"])  #[1, 224, 224]
    
    elev_tensor = preprocess_band(tile["ELEV"])  #[1, 224, 224]
    
    #concatenate: [3 + 1 + 1 + 1 = 6, 224, 224]
    full_tensor = torch.cat([rgb_tensor, ndvi_tensor, ndbi_tensor, elev_tensor], dim=0)
    
    return full_tensor  #[6, 224, 224]

def generate_tile_from_row(row,authenticate=False):
    """
    Builds a tile; a collection of arrays of the image bands for each coordinate pair.
    
    row: each row in the create_batches_from_df function iterations, pandas dataframe object
    """
    
    lat, lon = row["latitude"], row["longitude"]
    tile = {}
    np_tile = {}
    
    rgb, meta = generate_rgb_tile(lat, lon, step=0.01, authenticate=authenticate)
    ndvi, _ = generate_ndvi_tile(lat, lon, step=0.01)
    ndbi, _ = generate_ndbi_tile(lat, lon, step=0.01)
    elev, _ = generate_elevation_tile(lat, lon, step=0.01)
    
    tile['RGB'] = rgb
    tile['NDVI'] = ndvi
    tile['NDBI'] = ndbi
    tile['ELEV'] = elev
    tile["metadata"] = meta
    
        
    np_tile['RGB'] = np.array(rgb).astype(np.float32)
    np_tile['NDVI'] = np.array(ndvi).astype(np.float32)
    np_tile['NDBI'] = np.array(ndbi).astype(np.float32)
    np_tile['ELEV'] = np.array(elev).astype(np.float32)
    np_tile['LAT'] = row['latitude']
    np_tile['LON'] = row['longitude']
    return tile, np_tile
    
def create_batches_from_df(df, batch_size=64,authenticate=False):
    """
    Generates the final tensor output from input coordinates dataframe

    df: pandas dataframe
    """
    all_batches = []
    all_coords = []
    tile_df = []
    current_batch = []
    current_meta = []
    
    for _, row in tqdm.tqdm(df.iterrows(), total=len(df)):
        try:
            
            tile, _ = generate_tile_from_row(row,authenticate)
            tensor = tile_to_tensor(tile) #[6,224,224]
            if tensor.shape[0] != 6:
                print(f"Skipping ({row['latitude']}, {row['longitude']}) due to missing bands: {tensor.shape[0]}")
            else:
                current_batch.append(tensor)
                current_meta.append(tile["metadata"])
            if len(current_batch) == batch_size:
                batch_tensor = torch.stack(current_batch)
                all_batches.append(batch_tensor)
                all_coords.append(current_meta)
                current_batch = []
                current_meta = []

        except Exception as e:
            print(f"Skipping ({row['latitude']}, {row['longitude']}) due to error: {e}")

    if current_batch:
        batch_tensor = torch.stack(current_batch)
        all_batches.append(batch_tensor)
        all_coords.append(current_meta)
        
  
    return all_batches, all_coords


def save_chunk_torch(batches, coords, chunk_id, out_dir="chunks"):
    """
    saves batches and coordinates to directory
    """
    os.makedirs(out_dir, exist_ok=True)
    torch.save({
        "batches": batches,
        "coords": coords
    }, f"{out_dir}/tile_chunk_{chunk_id}.pt")








ee.Authenticate(auth_mode='notebook', force=True)
df = pd.read_csv(positive_ruins_coordinates_path)
df_chunk1 = df.iloc[:660]
batch1, coord1 = create_batches_from_df(df_chunk1)
save_chunk_torch(batch1, coord1, chunk_id=1)
shutil.make_archive("tile_chunks", 'zip', "chunks")
os.rename("tile_chunks.zip", "tile_chunks_part1.zip")



df = pd.read_csv(positive_ruins_coordinates_path)
df_chunk1 = df.iloc[660:1220]
batch1, coord1 = create_batches_from_df(df_chunk1)
save_chunk_torch(batch1, coord1, chunk_id=2)
shutil.make_archive("tile_chunks", 'zip', "chunks")
os.rename("tile_chunks.zip", "tile_chunks_part2.zip")


df = pd.read_csv(positive_ruins_coordinates_path)
df_chunk1 = df.iloc[1220:1880]
batch1, coord1 = create_batches_from_df(df_chunk1)
save_chunk_torch(batch1, coord1, chunk_id=3)
shutil.make_archive("tile_chunks", 'zip', "chunks")
os.rename("tile_chunks.zip", "tile_chunks_part3.zip")


df = pd.read_csv(positive_ruins_coordinates_path)
df_chunk1 = df.iloc[1880:2540]
batch1, coord1 = create_batches_from_df(df_chunk1)
save_chunk_torch(batch1, coord1, chunk_id=4)
shutil.make_archive("tile_chunks", 'zip', "chunks")
os.rename("tile_chunks.zip", "tile_chunks_part4.zip")


df = pd.read_csv(positive_ruins_coordinates_path)
df_chunk1 = df.iloc[2540:3200]
batch1, coord1 = create_batches_from_df(df_chunk1)
save_chunk_torch(batch1, coord1, chunk_id=5)
shutil.make_archive("tile_chunks", 'zip', "chunks")
os.rename("tile_chunks.zip", "tile_chunks_part5.zip")


df = pd.read_csv(positive_ruins_coordinates_path)
df_chunk1 = df.iloc[3200:3760]
batch1, coord1 = create_batches_from_df(df_chunk1)
save_chunk_torch(batch1, coord1, chunk_id=6)
shutil.make_archive("tile_chunks", 'zip', "chunks")
os.rename("tile_chunks.zip", "tile_chunks_part6.zip")


df = pd.read_csv('/kaggle/input/amazon-data/amazon_data.csv')
import shutil
df_chunk1 = df.iloc[3760:4420]
batch1, coord1 = create_batches_from_df(df_chunk1)
save_chunk_torch(batch1, coord1, chunk_id=7)
shutil.make_archive("tile_chunks", 'zip', "chunks")
os.rename("tile_chunks.zip", "tile_chunks_part7.zip")


df = pd.read_csv(positive_ruins_coordinates_path)
import shutil
df_chunk1 = df.iloc[4420:5080]
batch1, coord1 = create_batches_from_df(df_chunk1)
save_chunk_torch(batch1, coord1, chunk_id=8)
shutil.make_archive("tile_chunks", 'zip', "chunks")
os.rename("tile_chunks.zip", "tile_chunks_part8.zip")


df = pd.read_csv(positive_ruins_coordinates_path)
df_chunk1 = df.iloc[5080:5740]
batch1, coord1 = create_batches_from_df(df_chunk1)
save_chunk_torch(batch1, coord1, chunk_id=9)
shutil.make_archive("tile_chunks", 'zip', "chunks")
os.rename("tile_chunks.zip", "tile_chunks_part9.zip")


df = pd.read_csv(positive_ruins_coordinates_path)
df_chunk1 = df.iloc[5740:]
batch1, coord1 = create_batches_from_df(df_chunk1)
save_chunk_torch(batch1, coord1, chunk_id=10)
shutil.make_archive("tile_chunks", 'zip', "chunks")
os.rename("tile_chunks.zip", "tile_chunks_par10.zip")





## Extract

# Loading all the data that was previously saved to the directory#

# Ensuring no Residual RAM

# Excluding important libraries & variables from getting feleted
keep_vars = {
    'torch', 'gc', 'np', 'pd','best_model_state','nne_state','rgb_state', 
    'train_test_split','confusion_matrix', 'ConfusionMatrixDisplay', 'optim', 
    'resnet50', 'transforms', 'DataLoader', 'Dataset', 'nn', 'plt'}
for name in dir():
    if not name.startswith("_") and name not in keep_vars:
        del globals()[name]
        keep_vars = {
                    'torch', 'gc', 'np', 'pd','best_model_state','nne_state','rgb_state', 
                    'train_test_split','confusion_matrix', 'ConfusionMatrixDisplay', 'optim', 
                    'resnet50', 'transforms', 'DataLoader', 'Dataset', 'nn', 'plt'}
# Device 
device = 'cuda'
if device == 'cuda':
    torch.cuda.empty_cache()
else: 
    gc.collect()

## Paths
#Actual Ruins found in Kaggle
pt1 = '/kaggle/input/amazon-chunk-data/tile_chunk_1.pt'
pt2 = '/kaggle/input/amazon-chunk-data/tile_chunk_2.pt'
pt3 = '/kaggle/input/amazon-chunk-data/tile_chunk_3.pt'
pt4 = '/kaggle/input/amazon-chunk-data/tile_chunk_4.pt'
pt5 = '/kaggle/input/amazon-chunk-data/tile_chunk_5.pt'
pt6 = '/kaggle/input/amazon-chunk-data/tile_chunk_6.pt'
pt7 = '/kaggle/input/amazon-chunk-data/tile_chunk_7.pt'
pt8 = '/kaggle/input/amazon-chunk-data/tile_chunk_8.pt'
pt9 = '/kaggle/input/amazon-chunk-data/tile_chunk_9.pt'
pt10 = '/kaggle/input/amazon-chunk-data/tile_chunk_10.pt'

#Hand labeled not-ruins
plf1 = '/kaggle/input/ruins-true-negatives/labeled_false_ruins.pt'
plf2 = '/kaggle/input/labeled-false-ruins-2/labeled_false_ruins_2.pt'

#Tiles of interest
potential_ruins_1 = '/kaggle/input/potential-ruins/potential_ruins.pt'
potential_ruins_2 = '/kaggle/input/tp-chunk-2/tile_chunk_tp_2.pt'

# Loading 
t1 = torch.load(pt1,weights_only=False)
t2 = torch.load(pt2,weights_only=False)
t3 = torch.load(pt3,weights_only=False)
t4 = torch.load(pt4,weights_only=False)
t5 = torch.load(pt5,weights_only=False)
t6 = torch.load(pt6,weights_only=False)
t7 = torch.load(pt7,weights_only=False)
t8 = torch.load(pt8,weights_only=False)
t9 = torch.load(pt9,weights_only=False)
t10 = torch.load(pt10,weights_only=False)
f1 = torch.load(plf1,weights_only=False)
f2 = torch.load(plf2,weights_only=False)

# Features & Coords 
t1, t1_coord = torch.cat(t1['batches']),np.concatenate(t1['coords'])
t2, t2_coord = torch.cat(t2['batches']),np.concatenate(t2['coords'])
t3, t3_coord = torch.cat(t3['batches']),np.concatenate(t3['coords'])
t4, t4_coord = torch.cat(t4['batches']),np.concatenate(t4['coords'])
t5, t5_coord = torch.cat(t5['batches']),np.concatenate(t5['coords'])
t6, t6_coord = torch.cat(t6['batches']),np.concatenate(t6['coords'])
t7, t7_coord = torch.cat(t7['batches']),np.concatenate(t7['coords'])
t8, t8_coord = torch.cat(t8['batches']),np.concatenate(t8['coords'])
t9, t9_coord = torch.cat(t9['batches']),np.concatenate(t9['coords'])
t10, t10_coord = torch.cat(t10['batches']),np.concatenate(t10['coords'])
f1, f1_coord = torch.cat(f1['batches']),np.concatenate(f1['coords'])
f2, f2_coord = torch.cat(f2['batches']),np.concatenate(f2['coords'])


# Dataset 
true_ruins_batches = torch.cat((t1,t2,t3,t4,t5,t6,t7,t8,t9,t10))
true_ruins_coords = np.concatenate((t1_coord, t2_coord,
                                      t3_coord, t4_coord,
                                      t5_coord, t6_coord,
                                      t7_coord, t8_coord,
                                      t9_coord, t10_coord))


false_ruins_batches = torch.cat((f1, f2))
false_ruins_coords = np.concatenate((f1_coord, f2_coord))

del t1,t2,t3,t4,t5,t6,t7,t8,t9,t10,f1,f2
del t1_coord, t2_coord, t3_coord, t4_coord, t5_coord, t6_coord, t7_coord, t8_coord, t9_coord, t10_coord, f1_coord, f2_coord
if device == 'cuda':
    torch.cuda.empty_cache()
else: 
    gc.collect()



# Current Band layout [R,G,B,NDVI,NDBI,Elevation]

# Making labels and creating full dataset
false_labels = torch.zeros(len(false_ruins_batches))
true_labels = torch.ones(len(true_ruins_batches))

x = torch.cat((true_ruins_batches,false_ruins_batches))
y = torch.cat((true_labels,false_labels))

del true_ruins_batches,false_ruins_batches
if device == 'cuda':
    torch.cuda.empty_cache()
else: 
    gc.collect()


# Dataset Class 

class Tensor_Dataset(Dataset):
    def __init__(self,data,labels=None,coord=None,transform=None):
        self.data = data
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self,idx):
        dset = self.data[idx]
        if self.labels is not None:
            lab = self.labels[idx]
            return dset, lab
        if self.transform:
            dset = self.transform(dset)
        return dset
        
        



## Train/Test/Val Split 


X_train, X_temp, y_train, y_temp = train_test_split(x,y, test_size = .2, random_state = 42, stratify = y)
X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size = .5, random_state = 42, stratify= y_temp)
train_set = Tensor_Dataset(X_train,y_train)
test_set = Tensor_Dataset(X_test,y_test)
val_set = Tensor_Dataset(X_val,y_val)

del x, y
if device == 'cuda':
    torch.cuda.empty_cache()
else: 
    gc.collect()

train = DataLoader(train_set, batch_size=64)
test = DataLoader(test_set, batch_size=64)
val = DataLoader(val_set, batch_size=64)


## Plotting


def display_plots(train_acc, valid_acc, train_loss, valid_loss, name=None):
    """
    Displays model performance plots

    train_acc, valid_acc, train_loss, valid_loss: lists of epoch, acc/loss
    """

    plt.figure(figsize=(10, 7))
    plt.plot(train_loss, color='tab:blue', linestyle='-', label='train loss')
    plt.plot(valid_loss, color='tab:red', linestyle='-', label='validataion loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    plt.figure(figsize=(10, 7))
    plt.plot(train_acc, color='tab:blue', linestyle='-', label='train accuracy')
    plt.plot(valid_acc, color='tab:red', linestyle='-', label='validataion accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()



## Training

def build_resnet(input_channels=6,device='cuda',optimizer='SGD',lr=1e-2):
    """
    Builds a resnet feature extractor based on the number of input channels
    
    input_channels: int
    """
    assert optimizer in ['SGD','Adam'], f'Optimizer type {optimzer} not supported.'
    device = torch.device(device)
    model = models.resnet50(pretrained = True)
    if input_channels != 3:
        
        #Pretrained resnet weights from torchvision
        pretrained_weights = model.conv1.weight.clone()
    
        #Replacing initial layer with new conv1, modifying input channels for this dataset
        model.conv1 = nn.Conv2d(
            in_channels=input_channels,
            out_channels=64,
            kernel_size=7,
            stride=2,
            padding=3,
            bias=False
        )
    
        #Initialize the new conv1 weights using pretrained weights
        with torch.no_grad():
            model.conv1.weight[:, :3] = pretrained_weights  #RGB
            model.conv1.weight[:, 3:] = pretrained_weights[:, :3] / 3.0  #NDVI, NDBI, Elevation
            
    num_classes = 2 #yes/no image is a ruin
    in_features = model.fc.in_features
    model.fc = nn.Linear(in_features,num_classes)
    model = model.to(device)

    #Adam or SGD+Momentum
    if optimizer == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=.9)
        scheduler = False
        
    else:
        
        optimizer = optim.Adam(model.parameters(), lr=lr)
        scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.1)
    

    criterion = nn.CrossEntropyLoss()
    
    return model, optimizer, scheduler, criterion, device


def train_resnet(model,optimizer,criterion,device,dataloader):
    """
    resnet training loop, forward and backward pass with weights update

    model: torch model
    optimizer: torch optimizer
    criterion: torch loss function, progress scoring method
    """
    model.train()
    total_running_loss = 0
    train_running_correct = 0
    print('Training')

    for x,y in tqdm(dataloader):
        
        x,y = x.to(device),y.long().to(device)
        #reset gradients
        optimizer.zero_grad()

        #forward pass
        output = model(x)

        #calculating loss
        loss = criterion(output,y)
    
        total_running_loss += loss.item()

        _,preds = torch.max(output.data,1)

        #correct class predictions
        train_running_correct += (preds==y).sum().item()

        # Calculate new weights
        loss.backward()

        # Apply new weights
        optimizer.step()
        
        

    train_loss = total_running_loss/len(dataloader)
    train_acc =  100.*train_running_correct/len(dataloader.dataset)   
    
    return train_loss, train_acc


def val_resnet(model,optimizer,criterion,device,dataloader,cm=False):
    """
    Validating loop, scoring & performance 

    model: torch model
    optimizer: torch optimizer
    criterion: torch loss function, progress scoring method
    """
    
    model.eval()
    print('Validating')
    total_running_loss = 0
    val_running_correct = 0
    cm_labels = []
    cm_preds = []
    
    with torch.no_grad():
        
        for x,y in tqdm(dataloader):
            
            x,y = x.to(device),y.long().to(device)
        
            #forward pass, no backward for validation
            output = model(x).to(device)
            
            loss = criterion(output,y)
            
            total_running_loss += loss.item()
    
            _,preds = torch.max(output.data,1)
    
            val_running_correct += (preds==y).sum().item()
            
            #for final test set:
            if cm:
                cm_preds.extend(preds.cpu().numpy())
                cm_labels.extend(y.cpu().numpy())
    
    
    val_loss = total_running_loss/len(dataloader)
    val_acc =  100.*val_running_correct/len(dataloader.dataset)                   

    #for final test set:
    if cm:
        cm = confusion_matrix(cm_labels,cm_preds)
        display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])
        return val_loss, val_acc, display
    else:
        return val_loss, val_acc

     
    

# Full Run Through
if __name__ == '__main__':
    device='cuda'
    train_loss, val_loss = [],[]
    train_acc, val_acc = [],[]
    
    
    model, optimizer, scheduler, criterion, device = build_resnet(input_channels=6,device=device, optimizer='SGD',lr=1e-2)

    
    #Initial val to see what pretrained resnet can do:
    initial_val_loss, initial_val_acc = val_resnet(model,optimizer,criterion,device,val)
    print(f"Initial Validation Loss: {initial_val_loss}, Initial Validation Accuracy: {initial_val_acc}")
    
    epochs = 5
    base_val_acc = 0
    for epoch in range(epochs):
        train_loss_epoch, train_acc_epoch = train_resnet(model,optimizer,criterion,device,train)
        val_loss_epoch, val_acc_epoch = val_resnet(model,optimizer,criterion,device,val)
        train_loss.append(train_loss_epoch)
        train_acc.append(train_acc_epoch)
        val_loss.append(val_loss_epoch)
        val_acc.append(val_acc_epoch)

        #updating the saved weights to get the highest accuracy model if it begins to overfit by the end of 5 epochs
        if val_acc_epoch > base_val_acc:
            best_model_state = model.state_dict()
            base_val_epoch = val_acc_epoch
            
        if scheduler:
            scheduler.step(val_loss_epoch)
     
        print(f"Epoch {epoch+1} of {epochs}: Train Accuracy: {train_acc_epoch}, Train Loss:{train_loss_epoch}, Validation Accuracy: {val_acc_epoch}, Validation Loss: {val_loss_epoch}")
    
    #Test run
    test_loss, test_acc, cm_display = val_resnet(model,optimizer,criterion,device,test,cm=True)
    
    
    display_plots(train_acc, val_acc, train_loss, val_loss)
    print(f"Final Test Accuracy: {test_acc}, Final Test Loss: {test_loss}")
    cm_display.plot(cmap='Blues')
    plt.title("Confusion Matrix on Test Set")
    plt.show()
    

    



# To Directory
torch.save(best_model_state, "resnet50_6ch_feature_extractor.pth")


# Building Feature Extractor from Pretrained Weights #
def prepare_feature_extractor_from_state_dict(state_dict, device='cuda'):
    """
    Creating the feature extractor based on the previously trained weights
    
    state_dict: Previsouly trained weights, torch tensor.
    device: device the environment is running on, str.
    """
    model = models.resnet50(pretrained=False)
    

    model.conv1 = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3, bias=False)
    

    in_features = model.fc.in_features
    model.fc = nn.Linear(in_features, 2)
    
    # Loading saved weights
    model.load_state_dict(state_dict)
    
    # Removing final classification layer to make it a feature extractor
    feature_extractor = nn.Sequential(*list(model.children())[:-1])
    feature_extractor = feature_extractor.to(device)
    feature_extractor.eval()
    
    return feature_extractor


def extract_features(feature_extractor, dataloader, device='cuda'):
    """
    Function to extraxt (N,2048) features from (Num_Batches, Batch_Size, 6, 224, 224) dataset
    
    feature_extractor: a torch model object
    
    dataloader: the dataset, torch dataloader object
    """
    features_list = []
    labels_list = []  

    with torch.no_grad():
        for batch in dataloader:
            
            #For unlabeled data (potential ruins)
            if isinstance(batch, (tuple, list)) and len(batch) == 1:
                x = batch[0]
                y = None
            #For labeled datasets x & y with or without the coords
            elif isinstance(batch, (tuple, list)):
                x, y = batch[0], batch[1]
                
            else:
                x = batch
                y = None

            if x.dim() == 3:
                x = x.unsqueeze(0)

            x = x.to(device)
            feats = feature_extractor(x)          # [B, 2048, 1, 1]
            feats = feats.view(feats.size(0), -1) #Flattened to [B, 2048]
            features_list.append(feats.cpu())

            if y is not None:
                y = y.cpu()
                if y.dim() == 0:
                    y = y.unsqueeze(0)
                labels_list.append(y)

    
    if features_list and labels_list:
        return torch.cat(features_list), torch.cat(labels_list)
    elif features_list:
        return torch.cat(features_list), None
    else:
        #Return None if blank items get passed through
        return None, None


#Reloading Again to build features


# Ensuring no Residual RAM 
keep_vars = {'torch', 'gc', 'np', 'pd','best_model_state','best_state','nne_state','rgb_state'}
for name in dir():
    if not name.startswith("_") and name not in keep_vars:
        del globals()[name]
        keep_vars = {'torch', 'gc', 'np', 'pd','best_state','best_model_state','nne_state','rgb_state'}

# Device 
device = 'cuda'
if device == 'cuda':
    torch.cuda.empty_cache()
else: 
    gc.collect()

# Paths 
pt1 = '/kaggle/input/amazon-chunk-data/tile_chunk_1.pt'
pt2 = '/kaggle/input/amazon-chunk-data/tile_chunk_2.pt'
pt3 = '/kaggle/input/amazon-chunk-data/tile_chunk_3.pt'
pt4 = '/kaggle/input/amazon-chunk-data/tile_chunk_4.pt'
pt5 = '/kaggle/input/amazon-chunk-data/tile_chunk_5.pt'
pt6 = '/kaggle/input/amazon-chunk-data/tile_chunk_6.pt'
pt7 = '/kaggle/input/amazon-chunk-data/tile_chunk_7.pt'
pt8 = '/kaggle/input/amazon-chunk-data/tile_chunk_8.pt'
pt9 = '/kaggle/input/amazon-chunk-data/tile_chunk_9.pt'
pt10 = '/kaggle/input/amazon-chunk-data/tile_chunk_10.pt'

plf1 = '/kaggle/input/ruins-true-negatives/labeled_false_ruins.pt'
plf2 = '/kaggle/input/labeled-false-ruins-2/labeled_false_ruins_2.pt'

potential_ruins_1 = '/kaggle/input/potential-ruins/potential_ruins.pt'
potential_ruins_2 = '/kaggle/input/tp-chunk-2/tile_chunk_tp_2.pt'

# Loading 
t1 = torch.load(pt1,weights_only=False)
t2 = torch.load(pt2,weights_only=False)
t3 = torch.load(pt3,weights_only=False)
t4 = torch.load(pt4,weights_only=False)
t5 = torch.load(pt5,weights_only=False)
t6 = torch.load(pt6,weights_only=False)
t7 = torch.load(pt7,weights_only=False)
t8 = torch.load(pt8,weights_only=False)
t9 = torch.load(pt9,weights_only=False)
t10 = torch.load(pt10,weights_only=False)
f1 = torch.load(plf1,weights_only=False)
f2 = torch.load(plf2,weights_only=False)

# Features & Coordinates 
t1, t1_coord = torch.cat(t1['batches']),np.concatenate(t1['coords'])
t2, t2_coord = torch.cat(t2['batches']),np.concatenate(t2['coords'])
t3, t3_coord = torch.cat(t3['batches']),np.concatenate(t3['coords'])
t4, t4_coord = torch.cat(t4['batches']),np.concatenate(t4['coords'])
t5, t5_coord = torch.cat(t5['batches']),np.concatenate(t5['coords'])
t6, t6_coord = torch.cat(t6['batches']),np.concatenate(t6['coords'])
t7, t7_coord = torch.cat(t7['batches']),np.concatenate(t7['coords'])
t8, t8_coord = torch.cat(t8['batches']),np.concatenate(t8['coords'])
t9, t9_coord = torch.cat(t9['batches']),np.concatenate(t9['coords'])
t10, t10_coord = torch.cat(t10['batches']),np.concatenate(t10['coords'])
f1, f1_coord = torch.cat(f1['batches']),np.concatenate(f1['coords'])
f2, f2_coord = torch.cat(f2['batches']),np.concatenate(f2['coords'])

if device == 'cuda':
    torch.cuda.empty_cache()
else: 
    gc.collect()


p1 = torch.load(potential_ruins_1,weights_only=False)
p2 = torch.load(potential_ruins_2,weights_only=False)

p1, p1_coord = torch.cat(p1['batches']),np.concatenate(p1['coords'])
p2, p2_coord = torch.cat(p2['batches']),np.concatenate(p2['coords'])

pruins = torch.cat((p1,p2))
pcoord = np.concatenate((p1_coord,p2_coord))

if device == 'cuda':
    torch.cuda.empty_cache()
else: 
    gc.collect()

# Dataset 
true_ruins_batches = torch.cat((t1,t2,t3,t4,t5,t6,t7,t8,t9,t10))
true_ruins_coords = np.concatenate((
                                      t1_coord, t2_coord,
                                      t3_coord, t4_coord,
                                      t5_coord, t6_coord,
                                      t7_coord, t8_coord,
                                      t9_coord, t10_coord))


false_ruins_batches = torch.cat((f1, f2))
false_ruins_coords = np.concatenate((f1_coord, f2_coord))


if device == 'cuda':
    torch.cuda.empty_cache()
else: 
    gc.collect()


#Restructuring coords
true_coords = torch.tensor([[d['lat'], d['lon']] for d in true_ruins_coords])
false_coords = torch.tensor([[d['lat'], d['lon']] for d in false_ruins_coords])
potential_coords = torch.tensor([[d['lat'], d['lon']] for d in pcoord])




# Model state in this case was already defined in the environment
best_state = best_model_state
#sanitation check
print(len(best_state))

# Building resnet feature extractor from the traiend weights
feature_extractor = prepare_feature_extractor_from_state_dict(best_state, device='cuda')



#Building labels, creating dataset objects
true_ruins = Tensor_Dataset(data=true_ruins_batches,labels=torch.ones(len(true_ruins_batches)),coord=true_ruins_coords)
false_ruins = Tensor_Dataset(data=false_ruins_batches,labels=torch.zeros(len(false_ruins_batches)),coord=false_ruins_coords)
potential_ruins = Tensor_Dataset(data=pruins,labels=None,coord=pcoord)



# Feature extraction
true_features, true_labels = extract_features(feature_extractor, true_ruins, device='cuda')
false_features, false_labels = extract_features(feature_extractor, false_ruins, device='cuda')
potential_features, _ = extract_features(feature_extractor, potential_ruins, device='cuda')
true_feat_tensor, true_lab_tensor = torch.tensor(true_features), torch.tensor(true_labels)
false_feat_tensor, false_lab_tensor = torch.tensor(false_features), torch.tensor(false_labels)
potential_feat_tensor = torch.tensor(potential_features)

# Saving
del true_ruins, false_ruins
if device == 'cuda':
    torch.cuda.empty_cache()
else: 
    gc.collect()
    
torch.save({
    'features': true_feat_tensor,
    'labels': true_lab_tensor,
    'coords': true_coords
}, 'resnet_true_features_train.pt')

torch.save({
    'features': false_feat_tensor,
    'labels': false_lab_tensor,
    'coords': false_coords
}, 'resnet_false_features_test.pt')

torch.save({
    'features': potential_feat_tensor,
    'coords': potential_coords,
}, 'resnet_potential_features_val.pt')

